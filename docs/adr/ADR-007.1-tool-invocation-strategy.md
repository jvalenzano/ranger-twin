# ADR-007.1: Three-Layer Tool Invocation Strategy

**Status:** Accepted  
**Supersedes:** ADR-007 (mode="ANY" approach - confirmed design limitation)  
**Date:** 2025-12-28  
**Decision Makers:** jvalenzano - Digital Twin Team  
**Category:** Agent Architecture & Reliability

---

## Context

### Background

ADR-007 specified `FunctionCallingConfig(mode="ANY")` for API-level tool enforcement, promising ~99.5% deterministic tool invocation. During implementation, we discovered this creates an **infinite loop** in conversational agents.

### Root Cause Analysis

`mode="ANY"` is interpreted literally by Gemini: **every response must contain a tool call**. This includes the final synthesis response, which the agent cannot produce because:

1. User asks question
2. Agent calls tool (forced by `mode="ANY"`) ✅
3. Tool returns data ✅
4. Agent attempts text synthesis → **BLOCKED** (mode="ANY" requires tool call)
5. Agent forced to call another tool → **INFINITE LOOP**

### Evidence

| Source | Finding |
|--------|---------|
| GitHub Issue #784 (google/adk-python) | "Custom Function Tool goes in infinite loop" |
| Google AI Forum (Aug 2025) | "Infinite tool call loop when setting function_calling_config to ANY mode" |
| Expert Panel Research | Confirmed: `mode="ANY"` designed for single-turn only |
| Local Testing | 50+ rapid tool calls without completion observed |

### Conclusion

`mode="ANY"` is a **design limitation**, not a bug. Google's documentation implicitly avoids this pattern—no examples show `mode="ANY"` with synthesis agents. The recommended approach is `mode="AUTO"` with explicit completion mechanisms.

---

## Decision

Adopt a **Three-Layer Enforcement Pattern** that achieves 99% tool invocation reliability without infinite loops.

```
┌─────────────────────────────────────────────────────────────────┐
│                 THREE-LAYER ENFORCEMENT                          │
├─────────────────────────────────────────────────────────────────┤
│  TIER 1: API Configuration                                       │
│  └─ mode="AUTO" (eliminates infinite loop)                       │
│  └─ Allows tool calls AND text synthesis                         │
│                                                                  │
│  TIER 2: Instruction Enforcement (~90% first-pass)               │
│  └─ ReAct pattern (THINK → CALL → REASON → RESPOND)              │
│  └─ Decision trees mapping queries to required tools             │
│  └─ Explicit "you MUST call tools" guidance                      │
│                                                                  │
│  TIER 3: Validation Layer (~99% combined)                        │
│  └─ Post-response tool invocation verification                   │
│  └─ Retry with enforcement prompt if tool not called             │
│  └─ Escalation to human review after max retries                 │
│  └─ Complete audit trail for federal compliance                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Rationale

### Why Three Layers?

| Approach | Reliability | Infinite Loop Risk | Federal Defensibility |
|----------|-------------|-------------------|----------------------|
| `mode="ANY"` alone | 99.5% | **HIGH** (confirmed) | N/A (unusable) |
| `mode="AUTO"` alone | ~70% | None | Low |
| `mode="AUTO"` + instructions | ~90% | None | Medium |
| **Three-layer (this ADR)** | **~99%** | **None** | **High** |

### Defense-in-Depth

The pattern follows security best practices: no single layer is responsible for enforcement. If Tier 2 fails (agent ignores instructions), Tier 3 catches it. This is more auditable than a single API flag.

### Federal Compliance Argument

> "The system ensures tool invocation through three independent mechanisms:
> 1. **API-layer:** `mode="AUTO"` enables tool calling
> 2. **Instruction-layer:** ReAct prompts guide tool selection (~90% effectiveness)
> 3. **Validation-layer:** Post-response verification with automatic retry achieves 99% enforcement
>
> Even if one layer fails, the others catch it. All tool invocations are logged with timestamps, parameters, and outcomes for complete audit trail."

---

## Implementation

### Tier 1: API Configuration

```python
from google.genai import types

TOOL_CONFIG = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="AUTO"  # NOT "ANY" - allows synthesis after tool calls
        # No allowed_function_names (only valid with mode="ANY")
    )
)

GENERATE_CONTENT_CONFIG = types.GenerateContentConfig(
    tool_config=TOOL_CONFIG,
    temperature=0.1,  # Low temperature for deterministic tool selection
)
```

### Tier 2: Instruction Pattern

```python
SPECIALIST_INSTRUCTION = """
You are the {agent_name} for RANGER forest recovery system.

## Reasoning Process (THINK → CALL → REASON → RESPOND)

**THINK:** Identify what data you need
{decision_tree}

**CALL:** Execute the appropriate tool
- You MUST call a tool before responding to domain questions
- The system validates tool invocation and will retry if skipped

**REASON:** Interpret the tool response
- Check status field (success/error/no_data)
- Extract confidence score and data sources
- Note any limitations

**RESPOND:** Ground your answer in tool data
- Include specific findings from the tool
- Cite confidence score
- Reference data source and date
- Provide actionable recommendations

## Critical Rules
- NEVER respond with general knowledge for domain questions
- ALWAYS call a tool first, then synthesize
- If tool returns error, acknowledge the limitation explicitly
"""
```

### Tier 3: Validation Layer

Located in `agents/shared/validation.py`:

```python
class ToolInvocationValidator:
    """
    Post-response validation ensuring tool invocation compliance.
    Achieves 99% reliability when combined with Tier 2 instructions.
    """
    
    def __init__(self, agent, max_retries: int = 2):
        self.agent = agent
        self.max_retries = max_retries
        self.tools_invoked_this_turn: List[str] = []
        self.tool_execution_log: List[Dict] = []
    
    async def invoke_with_enforcement(
        self,
        query: str,
        required_tools: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Invoke agent with guaranteed tool invocation or explicit failure.
        
        Returns:
            {
                "success": bool,
                "response": agent_response,
                "tools_invoked": List[str],
                "attempts": int,
                "validation_outcome": "PASSED" | "RETRY_SUCCEEDED" | "ESCALATED",
                "audit_trail": List[dict]
            }
        """
        # Full implementation in agents/shared/validation.py
        ...
```

### Agent Assembly Pattern

```python
from google.adk.agents import LlmAgent
from agents.shared.validation import ToolInvocationValidator
from agents.shared.classifier import QueryIntentClassifier

def create_specialist_agent(name, instruction, tools, tool_mappings):
    """Factory for three-layer enforced specialist agents."""
    
    # Create validator
    validator = ToolInvocationValidator(agent=None, max_retries=2)
    before_cb, after_cb, error_cb = validator.create_tracking_callbacks()
    
    # Create agent
    agent = LlmAgent(
        name=name,
        model="gemini-2.0-flash",
        instruction=instruction,
        tools=tools,
        generate_content_config=GENERATE_CONTENT_CONFIG,
        before_tool_callback=before_cb,
        after_tool_callback=after_cb,
        on_tool_error_callback=error_cb,
    )
    
    validator.agent = agent
    classifier = QueryIntentClassifier(tool_mappings)
    
    return agent, validator, classifier
```

---

## Agent-Specific Configuration

### Specialist Agents (mode="AUTO" + Validation)

| Agent | Tools | Default Required Tool |
|-------|-------|----------------------|
| Trail Assessor | classify_damage, evaluate_closure, prioritize_trails | classify_damage |
| Burn Analyst | assess_severity, classify_mtbs, validate_boundary | assess_severity |
| Cruising Assistant | recommend_methodology, estimate_volume, assess_salvage, analyze_csv_data | estimate_volume |
| NEPA Advisor | search_regulatory_documents, decide_pathway, generate_documentation_checklist, estimate_compliance_timeline | decide_pathway |

### Coordinator Agent (mode="AUTO" without Validation)

The Coordinator uses `mode="AUTO"` but **without** the validation layer because:
- Not all queries require tool invocation (routing, clarification)
- Coordinator delegates to specialists who have enforcement
- Flexibility needed for multi-agent orchestration

```python
coordinator = LlmAgent(
    name="RecoveryCoordinator",
    model="gemini-2.0-flash",
    instruction=COORDINATOR_INSTRUCTION,
    tools=[portfolio_triage, delegate_query],
    generate_content_config=GENERATE_CONTENT_CONFIG,
    # No validation wrapper - routing flexibility
)
```

---

## Consequences

### Positive

1. **No Infinite Loops** — `mode="AUTO"` allows text synthesis
2. **99% Reliability** — Three layers catch edge cases
3. **Federal Audit Trail** — Every tool invocation logged with timestamp
4. **Explicit Escalation** — Failed validation routes to human review
5. **Simpler Agent Structure** — Direct LlmAgent, no LoopAgent complexity
6. **Transparent Enforcement** — Validation logic is inspectable, not hidden in API

### Negative

1. **More Code** — Validation layer adds ~200 lines
2. **Retry Latency** — Failed first-pass adds ~2-3 seconds
3. **Monitoring Required** — Must track first-pass success rate

### Risks and Mitigations

| Risk | Probability | Mitigation |
|------|-------------|------------|
| First-pass rate drops below 90% | Low | Strengthen Tier 2 instructions |
| Retry rate exceeds 10% | Low | Review tool mappings, adjust decision trees |
| Escalation overwhelms reviewers | Very Low | Alert threshold; improve instructions |

---

## Metrics

### Technical Metrics

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| First-pass tool invocation | >90% | <85% |
| Combined success (with retry) | >99% | <95% |
| Escalation rate | <1% | >5% |
| Validation latency overhead | <100ms | >500ms |

### Monitoring

```python
# Log aggregation query (example)
SELECT 
    agent_name,
    COUNT(*) as total_queries,
    SUM(CASE WHEN validation_outcome = 'PASSED' THEN 1 ELSE 0 END) as first_pass,
    SUM(CASE WHEN validation_outcome = 'RETRY_SUCCEEDED' THEN 1 ELSE 0 END) as retry_success,
    SUM(CASE WHEN validation_outcome = 'ESCALATED' THEN 1 ELSE 0 END) as escalated
FROM ranger_validation_logs
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY agent_name
```

---

## Migration Path

### From ADR-007 to ADR-007.1

1. **Change `mode="ANY"` to `mode="AUTO"`** in all agents
2. **Remove `allowed_function_names`** (only valid with mode="ANY")
3. **Add validation layer** to specialist agents
4. **Update audit callbacks** with correct signature (`tool_response`)
5. **Test each agent** for infinite loop elimination

### Rollback

If issues arise, agents can operate with Tier 1 + Tier 2 only (remove validation wrapper). Reliability drops to ~90% but remains functional.

---

## References

- [Google ADK LoopAgent Documentation](https://google.github.io/adk-docs/agents/workflow-agents/loop-agents/)
- [GitHub Issue #784: Infinite Loop](https://github.com/google/adk-python/issues/784)
- [Gemini API Function Calling](https://ai.google.dev/gemini-api/docs/function-calling)
- [ADR-007: Original Tool Invocation Strategy](./ADR-007-tool-invocation-strategy.md) (superseded)
- [Implementation Guide](../operations/CLAUDE-CODE-ADR007.1-IMPLEMENTATION.md)
- Expert Panel Research (December 2025)

---

## Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 2025-12-28 | Supersede ADR-007 | `mode="ANY"` confirmed to cause infinite loops |
| 2025-12-28 | Adopt three-layer enforcement | Achieves 99% reliability without loops |
| 2025-12-28 | Direct LlmAgent for specialists | LoopAgent unnecessary for single-turn |
| 2025-12-28 | No validation for Coordinator | Routing flexibility required |

---

## Appendix: Error Handling Taxonomy

| Error Type | Response | User Message | Escalation |
|------------|----------|--------------|------------|
| Tool execution fails | Return error with confidence=0 | "Unable to retrieve data. Please try again." | No |
| Validation fails after retries | Log escalation, return partial | "Requires human verification." | Yes |
| Request timeout | Kill request, log | "Request timed out." | No |
| Safety violation (Gemini blocks) | Reject, log | "Cannot process this request." | Yes |

---

**Document Owner:** RANGER Architecture Team  
**Last Updated:** December 28, 2025  
**Status:** Accepted - Ready for Implementation
